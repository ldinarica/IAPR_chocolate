{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DFE9XdlrUGM"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lRamsl6rW91"
      },
      "source": [
        "# Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb6lPFu_rZvY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import OneCycleLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILApixgWsoiY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/project\")\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y3CldAzraYd"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK4a0oLvSOdO"
      },
      "outputs": [],
      "source": [
        "class CocoChocolateDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 img_dir: str,\n",
        "                 coco_json: str,\n",
        "\n",
        "                 class_names: list,\n",
        "                 img_size=(512,512),\n",
        "                 mode: str = 'train'):\n",
        "        coco = json.load(open(coco_json))\n",
        "        self.img_dir = img_dir\n",
        "        self.id2fn = {im['id']: im['file_name'] for im in coco['images']}\n",
        "        self.samples = []\n",
        "        for ann in coco['annotations']:\n",
        "            img_id = ann['image_id']\n",
        "            x, y, w, h = map(int, ann['bbox'])\n",
        "            cls = ann['category_id'] - 1\n",
        "            self.samples.append((img_id, (x, y, w, h), cls))\n",
        "\n",
        "        self.class_names = class_names\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self._setup_transforms()\n",
        "\n",
        "    def _setup_transforms(self):\n",
        "        if self.mode == 'train':\n",
        "            self.tf = T.Compose([\n",
        "                T.Resize(self.img_size),\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.RandomResizedCrop(self.img_size, scale=(0.6,1.0), ratio=(0.75,1.33)),\n",
        "                T.RandomRotation(15),\n",
        "                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "                T.RandomAffine(degrees=0, translate=(0.1,0.1), scale=(0.8,1.2), shear=10),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
        "                T.RandomErasing(p=0.5, scale=(0.02,0.2), ratio=(0.3,3.3))\n",
        "            ])\n",
        "        else:\n",
        "            self.tf = T.Compose([\n",
        "                T.Resize(self.img_size),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, (x, y, w, h), cls = self.samples[idx]\n",
        "        fn = self.id2fn[img_id]\n",
        "        img = Image.open(os.path.join(self.img_dir, fn)).convert('RGB')\n",
        "        patch = img.crop((x, y, x+w, y+h))\n",
        "        x_tensor = self.tf(patch)\n",
        "        y_label = torch.tensor(cls, dtype=torch.long)\n",
        "        return x_tensor, y_label, fn, (x, y, w, h)\n",
        "\n",
        "class RepeatDataset(Dataset):\n",
        "    \"\"\"Repeats a base dataset x times with augmentations\"\"\"\n",
        "    def __init__(self, ds: Dataset, times: int):\n",
        "        self.ds    = ds\n",
        "        self.times = times\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds) * self.times\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ds[idx % len(self.ds)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV9f9VW8ri1v"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxSQMp68rjwz"
      },
      "outputs": [],
      "source": [
        "class ChocolateNet(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(256,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512,512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc83pwR64muI",
        "outputId": "b58b5d23-6ee7-4b8b-b953-465640f8f884"
      },
      "outputs": [],
      "source": [
        "model=ChocolateNet()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBLOkZxPrtdh"
      },
      "source": [
        "# Train & Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "TTtsB1pqq8dF",
        "outputId": "ad6d9850-e1b1-49e4-90c1-f4d38d304560"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_(img_dir, coco_json, class_names,\n",
        "                       epochs=20, batch_size=32, lr=1e-3,\n",
        "                       val_split=0.2, device='cuda', repeat_times=5):\n",
        "    # full dataset\n",
        "    base_ds = CocoChocolateDataset(img_dir, coco_json, class_names, mode='train')\n",
        "    full_ds = RepeatDataset(base_ds, times=repeat_times)\n",
        "\n",
        "    n_val = int(len(full_ds) * val_split)\n",
        "    n_train = len(full_ds) - n_val\n",
        "    train_ds, val_ds = random_split(full_ds, [n_train, n_val])\n",
        "    print(len(val_ds),len(train_ds))\n",
        "    return train_ds, val_ds\n",
        "\n",
        "\n",
        "IMG_DIR    = \"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/train_annotated/train\"\n",
        "JSON_PATH  = \"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/train_annotated/train/_annotations.coco.json\"\n",
        "CLASS_NAMES = [f\"Type{i}\" for i in range(1,14)]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_ds, val_ds = train_and_evaluate_(\n",
        "    IMG_DIR, JSON_PATH, CLASS_NAMES,\n",
        "    epochs=15, batch_size=32, lr=1e-3,\n",
        "    val_split=0.2, device=device,repeat_times=4\n",
        ")\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# 1) Helper to un-normalize and show a tensor image\n",
        "inv_normalize = T.Normalize(\n",
        "    mean=[-m/s for m, s in zip([0.485,0.456,0.406],[0.229,0.224,0.225])],\n",
        "    std=[1/s for s in [0.229,0.224,0.225]]\n",
        ")\n",
        "\n",
        "def imshow_tensor(img_tensor, ax):\n",
        "    \"\"\"Undo normalization and plot.\"\"\"\n",
        "    img = inv_normalize(img_tensor).clamp(0,1)  # [C,H,W] in [0,1]\n",
        "    npimg = img.permute(1,2,0).cpu().numpy()\n",
        "    ax.imshow(npimg)\n",
        "    ax.axis('off')\n",
        "\n",
        "# 2) Function to plot random samples from a Subset\n",
        "def plot_subset(ds, name, class_names, n=4):\n",
        "    fig, axes = plt.subplots(1, n, figsize=(n*3, 3))\n",
        "    fig.suptitle(f\"{name} set samples\", fontsize=16)\n",
        "    for ax in axes:\n",
        "        idx = random.randrange(len(ds))\n",
        "        img_tensor, label, fn, bbox = ds[idx]\n",
        "        imshow_tensor(img_tensor, ax)\n",
        "        ax.set_title(f\"{class_names[label]}\\n{fn}\\n{bbox}\", fontsize=8)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
        "    plt.show()\n",
        "\n",
        "# 3) After you’ve created train_ds and val_ds:\n",
        "plot_subset(train_ds, \"Train\", CLASS_NAMES, n=4)\n",
        "plot_subset(val_ds,   \"Val\",   CLASS_NAMES, n=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn_hkPp-3AUz"
      },
      "outputs": [],
      "source": [
        "def mixup_data(x, y, alpha=0.4):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0))\n",
        "    mixed_x = lam*x + (1-lam)*x[idx]\n",
        "    y_a, y_b = y, y[idx]\n",
        "    return mixed_x, y_a, y_b, lam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTPeuaI1rsIN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# — your CocoChocolateDataset, RepeatDataset, ChocolateNet here —\n",
        "\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0))\n",
        "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
        "    y_a, y_b = y, y[idx]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def train(img_dir, coco_json, class_names,\n",
        "          epochs=20, batch_size=32, lr=1e-3,\n",
        "          val_split=0.2, device='cuda', repeat_times=4,\n",
        "          resume_ckpt: str = None):\n",
        "\n",
        "    # — dataset & loaders —\n",
        "    base_ds = CocoChocolateDataset(img_dir, coco_json, class_names, mode='train')\n",
        "    full_ds = RepeatDataset(base_ds, times=repeat_times)\n",
        "    n_val   = int(len(full_ds) * val_split)\n",
        "    n_train = len(full_ds) - n_val\n",
        "    train_ds, val_ds = random_split(full_ds, [n_train, n_val])\n",
        "    # switch val to eval transforms\n",
        "    val_ds.dataset.ds.mode = 'val'\n",
        "    val_ds.dataset.ds._setup_transforms()\n",
        "\n",
        "    # class weights\n",
        "    base_samples = train_ds.dataset.ds.samples\n",
        "    all_train_labels = [base_samples[i % len(base_samples)][2] for i in train_ds.indices]\n",
        "    counts = np.bincount(all_train_labels, minlength=len(class_names))\n",
        "    weights = 1.0 / (counts + 1e-6)\n",
        "    weights = weights / weights.sum() * len(class_names)\n",
        "    weight_tensor = torch.tensor(weights, device=device, dtype=torch.float32)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1, weight=weight_tensor)\n",
        "\n",
        "    # sampler\n",
        "    sample_weights = [weights[l] for l in all_train_labels]\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # model / optimizer / scheduler\n",
        "    model     = ChocolateNet(len(class_names)).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=epochs,\n",
        "        pct_start=0.1,\n",
        "        div_factor=10,\n",
        "    )\n",
        "\n",
        "    # optionally resume\n",
        "    start_epoch = 1\n",
        "    best_acc    = 0.0\n",
        "    if resume_ckpt and os.path.isfile(resume_ckpt):\n",
        "        ckpt = torch.load(resume_ckpt, map_location=device)\n",
        "        model.load_state_dict(ckpt['model_state'])\n",
        "        optimizer.load_state_dict(ckpt['optim_state'])\n",
        "        scheduler.load_state_dict(ckpt['sched_state'])\n",
        "        start_epoch = ckpt['epoch'] + 1\n",
        "        best_acc    = ckpt['best_acc']\n",
        "        print(f\"Resuming at epoch {start_epoch}, best_acc={best_acc:.4f}\")\n",
        "\n",
        "    # — training loop —\n",
        "    for epoch in range(start_epoch, epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for imgs, labels, *_ in train_loader:\n",
        "            # MixUp\n",
        "            mixed, la, lb, lam = mixup_data(imgs, labels, alpha=0.4)\n",
        "            mixed = mixed.to(device)\n",
        "            la, lb = la.to(device), lb.to(device)\n",
        "\n",
        "            # forward + loss\n",
        "            out = model(mixed)\n",
        "            loss = lam * criterion(out, la) + (1 - lam) * criterion(out, lb)\n",
        "\n",
        "            # backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item() * mixed.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # — validation —\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels, *_ in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                out = model(imgs)\n",
        "                val_loss += criterion(out, labels).item() * imgs.size(0)\n",
        "                preds = out.argmax(1).cpu().numpy()\n",
        "                y_pred.extend(preds)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc   = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        print(f\"[Epoch {epoch}/{epochs}] train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "        # checkpoint best\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state': model.state_dict(),\n",
        "                'optim_state': optimizer.state_dict(),\n",
        "                'sched_state': scheduler.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "            }, f\"best_checkpoint_epoch.pth\")\n",
        "\n",
        "    return model, val_loader\n",
        "\n",
        "def eval_model(model, val_loader, class_names, device='cuda',weights=\"best_checkpoint_epoch.pth\"):\n",
        "    # load best\n",
        "    ckpt = torch.load(weights, map_location=device)\n",
        "    model.load_state_dict(ckpt['model_state'])\n",
        "    model.to(device).eval()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, *_ in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            out  = model(imgs)\n",
        "            y_pred.extend(out.argmax(1).cpu().numpy())\n",
        "            y_true.extend(labels.numpy())\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(df_cm, cmap='Blues')\n",
        "    plt.xticks(range(len(class_names)), class_names, rotation=45, ha='right')\n",
        "    plt.yticks(range(len(class_names)), class_names)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, cm[i,j], ha='center', va='center',\n",
        "                     color='white' if cm[i,j]>cm.max()/2 else 'black')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df_cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wit1Ga1arxTW"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "5J1EDvdiryG5",
        "outputId": "fa49d465-c645-4f12-ccc2-17d5d4d6d7a8"
      },
      "outputs": [],
      "source": [
        "if __name__==\"__main__\":\n",
        "    IMG_DIR   = \"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/train_annotated/train\"\n",
        "    JSON_PATH = \"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/train_annotated/train/_annotations_square.coco.json\"\n",
        "    CLASS_NAMES = [\n",
        "        \"Amandina\",\"Arabia\",\"Comtesse\",\"Crème brulée\",\"Jelly Black\",\n",
        "        \"Jelly Milk\",\"Jelly White\",\"Noblesse\",\"Noir authentique\",\n",
        "        \"Passion au lait\",\"Stracciatella\",\"Tentation noir\",\"Triangolo\"\n",
        "    ]\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model, val_loader = train(\n",
        "        IMG_DIR, JSON_PATH, CLASS_NAMES,\n",
        "        epochs=15, batch_size=32, lr=1e-3,\n",
        "        val_split=0.2, device=device,\n",
        "        repeat_times=5,\n",
        "        resume_ckpt=\"best_checkpoint_epoch_13.pth\"\n",
        "    )\n",
        "    eval_model(model, val_loader, CLASS_NAMES, device=device,weights=\"best_checkpoint_epoch.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "npxcSoSxtZUw",
        "outputId": "82cab0c1-fee4-47ab-c008-e1ff44b57aa5"
      },
      "outputs": [],
      "source": [
        "eval_model(model, val_loader, CLASS_NAMES, device=device,weights=\"best_checkpoint_epoch_13.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM3dq6lwr1N6"
      },
      "source": [
        "# Run on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yreYwhNyuyI0",
        "outputId": "85716e0e-1fc4-4d6b-a6fa-3257288b7cc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.measure import label, regionprops\n",
        "from skimage.morphology import remove_small_objects\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.measure    import label, regionprops\n",
        "from skimage.morphology import remove_small_objects, binary_dilation, disk\n",
        "from skimage.feature    import peak_local_max\n",
        "from skimage.segmentation import watershed\n",
        "from scipy import ndimage as ndi\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.enc1 = conv_block(in_channels, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = conv_block(256, 512)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = conv_block(512, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = conv_block(256, 128)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = conv_block(128, 64)\n",
        "\n",
        "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "\n",
        "        b = self.bottleneck(self.pool(e3))\n",
        "\n",
        "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "\n",
        "        return torch.sigmoid(self.final(d1))\n",
        "\n",
        "class ChocolateNet(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(256,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512,512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# 2) The Pipeline Dataset\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.measure    import label, regionprops\n",
        "from skimage.morphology import remove_small_objects\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ChocolateInferenceDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 image_dir: str,\n",
        "                 seg_model: nn.Module,\n",
        "                 cls_model: nn.Module,\n",
        "                 class_names: list,\n",
        "                 device: torch.device,\n",
        "                 seg_size=(256, 256),\n",
        "                 cls_size=(512, 512),\n",
        "                 seg_thresh=0.5,\n",
        "                 min_blob_size=100):\n",
        "        self.image_dir   = image_dir\n",
        "        self.fnames      = sorted(os.listdir(image_dir))\n",
        "        self.seg_model   = seg_model.to(device).eval()\n",
        "        self.cls_model   = cls_model.to(device).eval()\n",
        "        self.class_names = class_names\n",
        "        self.device      = device\n",
        "\n",
        "        # **Use seg_tf for U-Net** and include the same normalization you used in training\n",
        "        self.seg_tf = T.Compose([\n",
        "            T.Resize(seg_size),\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "        # classifier transform remains as before\n",
        "        self.cls_tf = T.Compose([\n",
        "            T.Resize(cls_size),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485,0.456,0.406],\n",
        "                        [0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "        self.seg_thresh    = seg_thresh\n",
        "        self.min_blob_size = min_blob_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fnames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fnames[idx]\n",
        "        img   = Image.open(os.path.join(self.image_dir, fname)).convert(\"RGB\")\n",
        "        W,H   = img.size\n",
        "\n",
        "        # — 1) SEGMENTATION at low res —\n",
        "        x_small = self.seg_tf(img).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            out_small = self.seg_model(x_small)[0,0].cpu().numpy()  # e.g. 256×256\n",
        "\n",
        "        mask_small = out_small > self.seg_thresh\n",
        "        mask_small = remove_small_objects(mask_small, min_size=self.min_blob_size)\n",
        "\n",
        "        # — 2) Inflate tiny white blobs at low res —\n",
        "        labels_small = label(mask_small)\n",
        "        inflated_small = np.zeros_like(mask_small)\n",
        "        for prop in regionprops(labels_small):\n",
        "            region = (labels_small == prop.label)\n",
        "            if prop.area < 300:            # tune threshold\n",
        "                region = binary_dilation(region, disk(10))\n",
        "            inflated_small |= region\n",
        "\n",
        "        # — 3) Upsample to full size —\n",
        "        mask_full = np.array(\n",
        "            Image.fromarray((inflated_small*255).astype(np.uint8))\n",
        "                 .resize((W,H), Image.NEAREST)\n",
        "        ) > 0\n",
        "\n",
        "        # — 4) Split touching blobs with watershed —\n",
        "        # distance transform\n",
        "        dist = ndi.distance_transform_edt(mask_full)\n",
        "        coordinates = peak_local_max(\n",
        "            dist,\n",
        "            labels=mask_full,\n",
        "            footprint=np.ones((40,40)),\n",
        "            threshold_abs=25,\n",
        "        )\n",
        "\n",
        "        markers = np.zeros_like(mask_full, dtype=bool)\n",
        "        if coordinates.shape[0] > 0:\n",
        "          markers[tuple(coordinates.T)] = True\n",
        "        markers = ndi.label(markers)[0]\n",
        "\n",
        "        if markers.max() == 0:\n",
        "             if mask_full.sum() > 0:\n",
        "                 labels_ws = label(mask_full)\n",
        "             else:\n",
        "                 labels_ws = np.zeros_like(mask_full) # No objects found at all\n",
        "        else:\n",
        "             labels_ws = watershed(-dist, markers, mask=mask_full)\n",
        "\n",
        "\n",
        "        # Optional debug\n",
        "        fig, axes = plt.subplots(1,3,figsize=(12,4))\n",
        "        axes[0].imshow(mask_full, cmap='gray'); axes[0].set_title(\"Upsampled Mask\"); axes[0].axis('off')\n",
        "        axes[1].imshow(dist, cmap='magma');      axes[1].set_title(\"Distance\");      axes[1].axis('off')\n",
        "        axes[2].imshow(labels_ws, cmap='tab20'); axes[2].set_title(\"Watershed\");     axes[2].axis('off')\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        # — 5) Crop & classify each separate region —\n",
        "        results = []\n",
        "        # Iterate over unique labels in labels_ws (excluding background label 0)\n",
        "        for obj_id in np.unique(labels_ws)[1:]:\n",
        "            region = (labels_ws == obj_id)\n",
        "            # Find the bounding box of this specific region\n",
        "            minr, minc, maxr, maxc = regionprops(region.astype(int))[0].bbox\n",
        "            patch = img.crop((minc, minr, maxc, maxr))\n",
        "            x = self.cls_tf(patch).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                pred = self.cls_model(x).argmax(1).item()\n",
        "            results.append({\n",
        "                \"bbox\": (minc, minr, maxc-minc, maxr-minr),\n",
        "                \"pred\": self.class_names[pred]\n",
        "            })\n",
        "\n",
        "        return img, fname, results\n",
        "\n",
        "\n",
        "# 3) Usage + Visualization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_names = class_names = [\n",
        "    \"Amandina\",\n",
        "    \"Arabia\",\n",
        "    \"Comtesse\",\n",
        "    \"Crème brulée\",\n",
        "    \"Jelly Black\",\n",
        "    \"Jelly Milk\",\n",
        "    \"Jelly White\",\n",
        "    \"Noblesse\",\n",
        "    \"Noir authentique\",\n",
        "    \"Passion au lait\",\n",
        "    \"Stracciatella\",\n",
        "    \"Tentation noir\",\n",
        "    \"Triangolo\",\n",
        "\n",
        "]\n",
        "# instantiate models and load weights\n",
        "seg_model = UNet()\n",
        "seg_model.load_state_dict(torch.load(\"Bon_mask_model4_epoch25.pth\", map_location=device))\n",
        "cls_model = ChocolateNet(len(class_names))\n",
        "cls_model.load_state_dict(torch.load(\"best_checkpoint_epoch_13.pth\", map_location=device)[\"model_state\"])\n",
        "\n",
        "# dataset\n",
        "dataset = ChocolateInferenceDataset(\n",
        "    image_dir=\"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/test\",\n",
        "    seg_model=seg_model,\n",
        "    cls_model=cls_model,\n",
        "    class_names=class_names,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# iterate & display\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/test_results_v2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for img, fname, results in dataset:\n",
        "    fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
        "    ax.imshow(img); ax.set_title(fname); ax.axis('off')\n",
        "    dets = [r[\"pred\"] for r in results]\n",
        "    print(f\"{fname} → {dets}\")\n",
        "    for r in results:\n",
        "        x,y,w,h = r[\"bbox\"]\n",
        "        ax.add_patch(plt.Rectangle((x,y), w, h, fill=False, edgecolor='lime', lw=2))\n",
        "        ax.text(x, y-5, r[\"pred\"], color='white',\n",
        "                bbox=dict(facecolor='black', alpha=0.5), fontsize=8)\n",
        "    plt.show()\n",
        "    save_path = os.path.join(output_dir, fname)\n",
        "    fig.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "\n",
        "# Make DataFrame & save\n",
        "rows = []\n",
        "for img, fname, results in dataset:\n",
        "    image_id = os.path.splitext(fname)[0]\n",
        "    counts = {cn: 0 for cn in class_names}\n",
        "    for r in results:\n",
        "        counts[r[\"pred\"]] += 1\n",
        "    row = {\"id\": image_id}\n",
        "    row.update(counts)\n",
        "    rows.append(row)\n",
        "\n",
        "columns = [\n",
        "    \"id\",\n",
        "    \"Jelly White\",\n",
        "    \"Jelly Milk\",\n",
        "    \"Jelly Black\",\n",
        "    \"Amandina\",\n",
        "    \"Crème brulée\",\n",
        "    \"Triangolo\",\n",
        "    \"Tentation noir\",\n",
        "    \"Comtesse\",\n",
        "    \"Noblesse\",\n",
        "    \"Noir authentique\",\n",
        "    \"Passion au lait\",\n",
        "    \"Arabia\",\n",
        "    \"Stracciatella\",\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(rows, columns=columns)\n",
        "df[\"id\"]=df[\"id\"].apply(lambda x: x[2:])\n",
        "df.to_csv(\"submission.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Columns in this exact order:\", df.columns.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtwCHbbbI33y"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(rows, columns=columns)\n",
        "df[\"id\"]=df[\"id\"].apply(lambda x: x[1:])\n",
        "df.to_csv(\"submission.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB8oLfFdE7BN",
        "outputId": "4bc26df2-9ae1-4338-98e2-283f656744d4"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "oo4EH_XmyfNt",
        "outputId": "1bb4be11-3d92-4447-f40e-04e51c51f75c"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def load_test_image(image_path, size=(256, 256)):\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize(size)\n",
        "    transform = transforms.ToTensor()\n",
        "    image_tensor = transform(image).unsqueeze(0)  # shape: [1, 3, H, W]\n",
        "    return image_tensor, image\n",
        "def predict_mask(model, image_tensor, threshold=0.5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        output = model(image_tensor)\n",
        "        pred_mask = (output.squeeze(0).squeeze(0) > threshold).float().cpu()\n",
        "    return pred_mask\n",
        "def show_result(original_img, pred_mask):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original_img)\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(pred_mask, cmap=\"gray\")\n",
        "    plt.title(\"Predicted Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# Replace with your test image path\n",
        "test_image_path = \"dataset_project_iapr2025/test/L1000839.JPG\"\n",
        "\n",
        "# Load model (if starting fresh)\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load(\"tout5.pth\", map_location=device))\n",
        "\n",
        "# Inference\n",
        "image_tensor, original_img = load_test_image(test_image_path, size=(256, 256))\n",
        "predicted_mask = predict_mask(model, image_tensor)\n",
        "show_result(original_img, predicted_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRUs0EXE8rrn"
      },
      "source": [
        "# Backup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thV5LkiG1ncP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from scipy import ndimage as ndi\n",
        "from skimage.measure      import label, regionprops\n",
        "from skimage.morphology   import remove_small_objects, binary_dilation, disk\n",
        "from skimage.segmentation import watershed\n",
        "from torchvision import transforms as T\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# -------------------------\n",
        "# 1) Model Definitions\n",
        "# -------------------------\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super().__init__()\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.enc1 = conv_block(in_channels, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.bottleneck = conv_block(256, 512)\n",
        "        self.up3 = nn.ConvTranspose2d(512,256,2,2)\n",
        "        self.dec3 = conv_block(512,256)\n",
        "        self.up2 = nn.ConvTranspose2d(256,128,2,2)\n",
        "        self.dec2 = conv_block(256,128)\n",
        "        self.up1 = nn.ConvTranspose2d(128,64,2,2)\n",
        "        self.dec1 = conv_block(128,64)\n",
        "        self.final = nn.Conv2d(64,out_channels,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "        b  = self.bottleneck(self.pool(e3))\n",
        "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "        return torch.sigmoid(self.final(d1))\n",
        "\n",
        "\n",
        "class ChocolateNet(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(256,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512,512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# 2) Inference Dataset with Watershed\n",
        "# --------------------------------------\n",
        "class ChocolateInferenceDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 image_dir,\n",
        "                 seg_model: nn.Module,\n",
        "                 cls_model: nn.Module,\n",
        "                 class_names: list,\n",
        "                 device: torch.device,\n",
        "                 seg_size=(256,256),\n",
        "                 cls_size=(256,256),\n",
        "                 seg_thresh=0.5,\n",
        "                 min_blob_size=100):\n",
        "        self.image_dir   = image_dir\n",
        "        self.fnames      = sorted(os.listdir(image_dir))\n",
        "        self.seg_model   = seg_model.to(device).eval()\n",
        "        self.cls_model   = cls_model.to(device).eval()\n",
        "        self.class_names = class_names\n",
        "        self.device      = device\n",
        "\n",
        "        # exactly the same normalization you used for U-Net training\n",
        "        self.seg_tf = T.Compose([\n",
        "            T.Resize(seg_size),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485,0.456,0.406],\n",
        "                        [0.229,0.224,0.225])\n",
        "        ])\n",
        "        self.cls_tf = T.Compose([\n",
        "            T.Resize(cls_size),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485,0.456,0.406],\n",
        "                        [0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "        self.seg_thresh    = seg_thresh\n",
        "        self.min_blob_size = min_blob_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fnames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load\n",
        "        fname = self.fnames[idx]\n",
        "        img   = Image.open(os.path.join(self.image_dir, fname)).convert(\"RGB\")\n",
        "        W,H   = img.size\n",
        "\n",
        "        # 1) U-Net segmentation @ low res\n",
        "        with torch.no_grad():\n",
        "            x_small = self.seg_tf(img).unsqueeze(0).to(self.device)\n",
        "            out_small = self.seg_model(x_small)[0,0].cpu().numpy()\n",
        "\n",
        "        # 2) Binarize + remove tiny specks\n",
        "        mask_small = out_small > self.seg_thresh\n",
        "        mask_small = remove_small_objects(mask_small, min_size=self.min_blob_size)\n",
        "\n",
        "        # 3) Inflate only the very small blobs at low res\n",
        "        labels_small = label(mask_small)\n",
        "        inflated_small = np.zeros_like(mask_small)\n",
        "        for prop in regionprops(labels_small):\n",
        "            region = (labels_small == prop.label)\n",
        "            if prop.area < 300:              # tune this small-area cutoff\n",
        "                region = binary_dilation(region, disk(8))\n",
        "            inflated_small |= region\n",
        "\n",
        "        # 4) Upsample to original resolution\n",
        "        mask_full = np.array(\n",
        "            Image.fromarray((inflated_small*255).astype(np.uint8))\n",
        "                 .resize((W,H), Image.NEAREST)\n",
        "        ) > 0\n",
        "\n",
        "        # 5) Centroid‐seeded watershed to split touching blobs\n",
        "        labels0 = label(mask_full)\n",
        "        dist    = ndi.distance_transform_edt(mask_full)\n",
        "        markers = np.zeros_like(labels0)\n",
        "        for i, prop in enumerate(regionprops(labels0), start=1):\n",
        "            cy, cx = prop.centroid\n",
        "            markers[int(cy), int(cx)] = i\n",
        "        labels_ws = watershed(-dist, markers, mask=mask_full)\n",
        "\n",
        "        # 6) Crop & classify each region\n",
        "        results = []\n",
        "        for region in regionprops(labels_ws):\n",
        "            minr, minc, maxr, maxc = region.bbox\n",
        "            patch = img.crop((minc, minr, maxc, maxr))\n",
        "            x = self.cls_tf(patch).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                pred = self.cls_model(x).argmax(1).item()\n",
        "            results.append({\n",
        "                \"bbox\": (minc, minr, maxr-minr, maxc-minc),\n",
        "                \"pred\": self.class_names[pred]\n",
        "            })\n",
        "\n",
        "        return img, fname, results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3) Usage + Visualization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_names = class_names = [\n",
        "    \"Amandina\",\n",
        "    \"Arabia\",\n",
        "    \"Comtesse\",\n",
        "    \"Crème brulée\",\n",
        "    \"Jelly Black\",\n",
        "    \"Jelly Milk\",\n",
        "    \"Jelly White\",\n",
        "    \"Noblesse\",\n",
        "    \"Noir authentique\",\n",
        "    \"Passion au lait\",\n",
        "    \"Stracciatella\",\n",
        "    \"Tentation noir\",\n",
        "    \"Triangolo\",\n",
        "\n",
        "]\n",
        "# instantiate models and load weights\n",
        "seg_model = UNet()\n",
        "seg_model.load_state_dict(torch.load(\"Bon_mask_model4_epoch25.pth\", map_location=device))\n",
        "cls_model = ChocolateNet(len(class_names))\n",
        "cls_model.load_state_dict(torch.load(\"best_choco_cnn_square.pth\", map_location=device))\n",
        "\n",
        "# dataset\n",
        "dataset = ChocolateInferenceDataset(\n",
        "    image_dir=\"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/test\",\n",
        "    seg_model=seg_model,\n",
        "    cls_model=cls_model,\n",
        "    class_names=class_names,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# iterate & display\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/project/dataset_project_iapr2025/test_results_v2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# for img, fname, results in dataset:\n",
        "#     fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
        "#     ax.imshow(img); ax.set_title(fname); ax.axis('off')\n",
        "#     dets = [r[\"pred\"] for r in results]\n",
        "#     print(f\"{fname} → {dets}\")\n",
        "#     for r in results:\n",
        "#         x,y,w,h = r[\"bbox\"]\n",
        "#         ax.add_patch(plt.Rectangle((x,y), w, h, fill=False, edgecolor='lime', lw=2))\n",
        "#         ax.text(x, y-5, r[\"pred\"], color='white',\n",
        "#                 bbox=dict(facecolor='black', alpha=0.5), fontsize=8)\n",
        "#     plt.show()\n",
        "#     save_path = os.path.join(output_dir, fname)\n",
        "#     fig.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "#     plt.close(fig)\n",
        "\n",
        "# Make DataFrame & save\n",
        "rows = []\n",
        "for img, fname, results in dataset:\n",
        "    image_id = os.path.splitext(fname)[0]\n",
        "    counts = {cn: 0 for cn in class_names}\n",
        "    for r in results:\n",
        "        counts[r[\"pred\"]] += 1\n",
        "    row = {\"id\": image_id}\n",
        "    row.update(counts)\n",
        "    rows.append(row)\n",
        "\n",
        "columns = [\n",
        "    \"id\",\n",
        "    \"Jelly White\",\n",
        "    \"Jelly Milk\",\n",
        "    \"Jelly Black\",\n",
        "    \"Amandina\",\n",
        "    \"Crème brulée\",\n",
        "    \"Triangolo\",\n",
        "    \"Tentation noir\",\n",
        "    \"Comtesse\",\n",
        "    \"Noblesse\",\n",
        "    \"Noir authentique\",\n",
        "    \"Passion au lait\",\n",
        "    \"Arabia\",\n",
        "    \"Stracciatella\",\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(rows, columns=columns)\n",
        "df[\"id\"]=df[\"id\"].apply(lambda x: x[2:])\n",
        "df.to_csv(\"submission.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Columns in this exact order:\", df.columns.tolist())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
